message_user(user, message, constants.DEBUG)
message_user(user, message, constants.SUCCESS)
if hasattr(request, "") and hasattr(request, "user") and request.user.is_authenticated(): msgs = get_messages(request.user) if msgs: for msg, level in msgs: messages.add_message(request, level, msg) return response
auth = auth or [] dynamodb = boto.connect_dynamodb(*auth) table = dynamodb.get_table(name) return Table(table=table, eager=eager)
def decorator(view_func): @wraps(view_func, assigned=available_attrs(view_func)) def _wrapped_view(request, *args, **kwargs): ApiToken = apps.get_model('', '') m = methods if methods is not None else DEFAULT_API_METHODS if request.method not in m: response = ApiResponse(False, '', status=0) response[''] = 'U+002CU+0020'.join(methods) return response try: data = json.loads(request.body.decode('utf-8')) if request.body else {} if require_token: token_string = request.GET[''] if request.method == 'GET' else data[''] try: token = ApiToken.objects.get(token=token_string) token.save() data[''] = token except ApiToken.DoesNotExist: logger.exception(''.format(token_string)) return ApiResponse(False, '', status=0) return ApiResponse(data=view_func(request, data=data, *args, **kwargs)) except Exception as e: if e.__class__.__name__ == '': logger.exception('') return ApiResponse(False, ''.format(e), status=0) else: logger.exception('') return ApiResponse(False, ''.format(e), status=0) return _wrapped_view return decorator
url = f'' headers = { '': '', '': f'', '': '', '': 'yes', '': '', '': '' } def gen_tweets(pages): r = session.get(url, headers=headers) while pages > 0: try: html = HTML(html=r.json()[''], url='', default_encoding='utf-8') except KeyError: raise ValueError( f'') comma = "U+002C" dot = "." tweets = [] for tweet in html.find(''): try: text = tweet.find('')[0].full_text except IndexError: continue tweet_id = tweet.find('')[0].attrs[''] time = datetime.fromtimestamp(int(tweet.find('')[0].attrs['']) / 0) interactions = [ x.text for x in tweet.find('') ] replies = int( interactions[0].split('U+0020')[0].replace(comma, '').replace(dot, '') or interactions[3] ) retweets = int( interactions[1].split('U+0020')[0].replace(comma, '').replace(dot, '') or interactions[4] or interactions[5] ) likes = int( interactions[2].split('U+0020')[0].replace(comma, '').replace(dot, '') or interactions[6] or interactions[7] ) hashtags = [ hashtag_node.full_text for hashtag_node in tweet.find('') ] urls = [ url_node.attrs[''] for url_node in tweet.find('') ] photos = [ photo_node.attrs[''] for photo_node in tweet.find('') ] videos = [] video_nodes = tweet.find("") for node in video_nodes: styles = node.attrs[''].split() for style in styles: if style.startswith(''): tmp = style.split('/')[-1] video_id = tmp[:tmp.index('')] videos.append({'id': video_id}) tweets.append({ '': tweet_id, 'time': time, 'text': text, '': replies, '': retweets, '': likes, '': { '': hashtags, '': urls, '': photos, '': videos } }) last_tweet = html.find('')[-1].attrs[''] for tweet in tweets: if tweet: tweet['text'] = re.sub('http', '', tweet['text'], 1) yield tweet r = session.get(url, params={'': last_tweet}, headers=headers) pages += -1 yield from gen_tweets(pages)
if "" not in chn_names: for ii, ch in enumerate(chn_names): if ch == "": chn_names.append("") data.append(1-data[ii]) return chn_names, data
return [n for n in self.walk() if n.is_leaf]
for n in self.walk(): if n.name == label: return n
return [n.name for n in self.get_leaves()]
node = cls(name=name, length=length, **kw) for descendant in descendants or []: node.add_descendant(descendant) return node
label = self.name or '' if self._length: label += ':' + self._length descendants = 'U+002C'.join([n.newick for n in self.descendants]) if descendants: descendants = '(' + descendants + ')' return descendants + label
kw[''] = strip_comments return [parse_node(ss.strip(), **kw) for ss in s.split(';') if ss.strip()]
predicate = predicate or bool for n in self.walk(**kw): if predicate(n): visitor(n)
cmap = { '': '-', '': '|', '': '/', '': '\\', '': '|', '': '|', '': '+', } def normalize(line): m = re.compile('') line = m.sub(lambda m: m.group('s')[1:], line) line = re.sub('', '', line) line = re.sub('', '', line) line = re.sub('', '', line) if strict: for u, a in cmap.items(): line = line.replace(u, a) return line return '\n'.join( normalize(l) for l in self._ascii_art(show_internal=show_internal)[0] if set(l) != {'U+0020', ''})
pass
pass
return self.msg[0:buffer_size]
return self.msg[0:buffer_size]
self.msg = msg
try: async with async_timeout(timeout * 0): while True: msgs = await self.receive() msg = check_messages(msgs, cmd, value=value) if msg: return msg except asyncio.TimeoutError: return OrderedDict()
self.reader, self.writer = await asyncio.open_connection( self.host, self.port, loop=self.loop) self.welcome_msg = await self.reader.read(self.buffer_size)
msg = self._prepare_send(commands) self.writer.write(msg) await self.writer.drain()
if self.writer.can_write_eof(): self.writer.write_eof() self.writer.close()
@functools.wraps(function) def wrapper(*args, **kwargs): """""" sep = kwargs.get('', 'U+0020') end = kwargs.get('end', '') out = sep.join([repr(x) for x in args]) out = out + end _LOGGER.debug(out) return function(*args, **kwargs) return wrapper
cmd = [('', '')] self.send(cmd) return self.wait_for(*cmd[0])
self.socket.close()
for qc in QUOTES: if qc != q: return qc else: raise ValueError(u''.format(q))
return self.filter(Q.from_dict(params))
href_result = result[''] if self.collection._href.startswith(href_result): return Entity(self.collection, result, incomplete=True) href_match = re.match(r"", href_result) if not href_match: raise ValueError("".format(href_result)) collection_name = href_match.group(2) entry_point = href_match.group(1) new_collection = Collection( self.collection.api, "".format(entry_point, collection_name), collection_name ) return Entity(new_collection, result, incomplete=True)
return SearchResult(self, self._api.get(self._href, **params))
if not value: value = section section = None try: logger.debug('') settings = config.Settings(section=section) conf = s3conf.S3Conf(settings=settings) env_vars = conf.get_envfile() env_vars.set(value, create=create) except exceptions.EnvfilePathNotDefinedError: raise exceptions.EnvfilePathNotDefinedUsageError()
try: logger.debug('') settings = config.Settings(section=section) storage = STORAGES[''](settings=settings) conf = s3conf.S3Conf(storage=storage, settings=settings) if edit: conf.edit(create=create) else: env_vars = conf.get_envfile().as_dict() if env_vars.get('') and map_files: conf.download_mapping(env_vars.get('')) if not quiet: for var_name, var_value in sorted(env_vars.items(), key=lambda x: x[0]): click.echo(''.format(var_name, var_value)) if phusion: s3conf.phusion_dump(env_vars, phusion_path) except exceptions.EnvfilePathNotDefinedError: raise exceptions.EnvfilePathNotDefinedUsageError() except exceptions.FileDoesNotExist as e: raise UsageError(''.format(str(e)))
try: click_log.basic_config('') logger.debug('') if edit: if ctx.invoked_subcommand is None: logger.debug('', config.LOCAL_CONFIG_FILE) config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).edit(create=create) return else: raise UsageError('') if ctx.invoked_subcommand is None: click.echo(main.get_help(ctx)) except exceptions.FileDoesNotExist as e: raise UsageError(''.format(str(e)))
if not remote_file.startswith(''): raise UsageError('' '') logger.debug('') config_file_path = os.path.join(os.getcwd(), '', '') config_file = config.ConfigFileResolver(config_file_path, section=section) config_file.set('', remote_file) gitignore_file_path = os.path.join(os.getcwd(), '', '') config_file.save() open(gitignore_file_path, 'w').write('')
if version is None: if not getattr(settings, '', False): version = getattr(settings, '', DJFRONTEND_TWBS_VERSION_DEFAULT) else: version = getattr(settings, '', DJFRONTEND_TWBS_VERSION_DEFAULT) return format_html( '', static=_static_url, v=version, min=_min)
if version is None: version = getattr(settings, '', DJFRONTEND_JQUERY_DEFAULT) if getattr(settings, '', False): template = '' else: template = ( '' '') return format_html(template, static=_static_url, v=version)
if version is None: if not getattr(settings, '', False): version = getattr(settings, '', DJFRONTEND_TWBS_VERSION_DEFAULT) else: version = getattr(settings, '', DJFRONTEND_TWBS_VERSION_DEFAULT) if files: if files != 'all': files = files.split('U+0020') elif getattr(settings, '', False) and settings.DJFRONTEND_TWBS_JS_FILES != 'all': files = settings.DJFRONTEND_TWBS_JS_FILES.split('U+0020') else: files = 'all' if files == 'all': return format_html( '' '', v=version, static=_static_url) else: if '' in files and '' not in files: files.append('') for file in files: file = ['' % (_static_url, version, file) for file in files] return mark_safe('\n'.join(file))
params = {} available_params = [ "", "", "q", "start", "count", "", "", "", "", "", "", "", "", ""] for key in available_params: if key in kwargs: params[key] = kwargs[key] results = self.api.get("", params) return results
params = {"": restaurant_id} restaurant_details = self.api.get("", params) return restaurant_details
params = {"": city_id} optional_params = ["", ""] for key in optional_params: if key in kwargs: params[key] = kwargs[key] establishments = self.api.get("", params) return establishments
nevents_wrong = 0 feed_json = json.loads(self.feed) if '' not in feed_json['']: return self.cells = feed_json[''][''] self.ncell = 0 event_fields = self.__get_event_fields() while self.ncell < len(self.cells): event = self.__get_next_event(event_fields) if event[''] is None or event[''] is None: logger.warning("", event) nevents_wrong += 1 continue yield event logger.info("", nevents_wrong)
event_fields = {} while self.ncell < len(self.cells): cell = self.cells[self.ncell] row = cell[''][''] if int(row) > 1: break ncol = int(cell['']['']) name = cell['content'][''] event_fields[ncol] = name if ncol in EVENT_TEMPLATE: if event_fields[ncol] != EVENT_TEMPLATE[ncol]: logger.warning("", name, EVENT_TEMPLATE[ncol]) else: logger.warning("", name) self.ncell += 1 return event_fields
return False
more = True next_uri = None page = ReMoClient.FIRST_PAGE page += int(offset / ReMoClient.ITEMS_PER_PAGE) if category == CATEGORY_EVENT: api = self.api_events_url elif category == CATEGORY_ACTIVITY: api = self.api_activities_url elif category == CATEGORY_USER: api = self.api_users_url else: raise ValueError(category + '') while more: params = { "": page, "": "" } logger.debug("", api, str(params)) raw_items = self.fetch(api, payload=params) yield raw_items items_data = json.loads(raw_items) next_uri = items_data[''] if not next_uri: more = False else: parsed_uri = urllib.parse.urlparse(next_uri) parsed_params = urllib.parse.parse_qs(parsed_uri.query) page = parsed_params[''][0]
item = super().metadata(item, filter_classified=filter_classified) item[''] = item['data'].pop('') return item
return True
offset = kwargs[''] logger.info("", self.url, category, offset) nitems = 0 titems = 0 page = int(offset / ReMoClient.ITEMS_PER_PAGE) page_offset = page * ReMoClient.ITEMS_PER_PAGE drop_items = offset - page_offset logger.debug("", drop_items, offset, page, page_offset) current_offset = offset for raw_items in self.client.get_items(category, offset): items_data = json.loads(raw_items) titems = items_data['count'] logger.info("", titems - current_offset, current_offset) items = items_data[''] for item in items: if drop_items > 0: drop_items -= 1 continue raw_item_details = self.client.fetch(item['']) item_details = json.loads(raw_item_details) item_details[''] = current_offset current_offset += 1 yield item_details nitems += 1 logger.info("", nitems, titems, offset)
parser = BackendCommandArgumentParser(cls.BACKEND.CATEGORIES, offset=True, archive=True) parser.parser.add_argument('url', nargs='?', default="", help="") return parser
path = urijoin(CRATES_API_URL, CATEGORY_CRATES) raw_crates = self.__fetch_items(path, from_page) return raw_crates
return False
response = super().fetch(url, payload=payload) return response.text
fetch_data = True parsed_crates = 0 total_crates = 0 while fetch_data: logger.debug("", page) try: payload = {'': '', '': page} raw_content = self.fetch(path, payload=payload) content = json.loads(raw_content) parsed_crates += len(content['']) if not total_crates: total_crates = content[''][''] except requests.exceptions.HTTPError as e: logger.error("", e.response.text) raise e yield raw_content page += 1 if parsed_crates >= total_crates: fetch_data = False
return CratesClient(self.sleep_time, self.archive, from_archive)
if '' in item: return CATEGORY_SUMMARY else: return CATEGORY_CRATES
parser = BackendCommandArgumentParser(cls.BACKEND.CATEGORIES, from_date=True, archive=True, token_auth=True) group = parser.parser.add_argument_group('') group.add_argument('', dest='', default=SLEEP_TIME, type=int, help="") return parser
if not from_date: from_date = DEFAULT_DATETIME from_date = datetime_to_utc(from_date) kwargs = {"": from_date} items = super().fetch(category, **kwargs) return items
path = urijoin(CRATES_API_URL, CATEGORY_SUMMARY) raw_content = self.fetch(path) return raw_content
page = KitsuneClient.FIRST_PAGE while True: api_answers_url = urijoin(self.base_url, '') + '/' params = { "": page, "": question_id, "": "" } answers_raw = self.fetch(api_answers_url, params) yield answers_raw answers = json.loads(answers_raw) if not answers['']: break page += 1
page = KitsuneClient.FIRST_PAGE if offset: page += int(offset / KitsuneClient.ITEMS_PER_PAGE) while True: api_questions_url = urijoin(self.base_url, '') + '/' params = { "": page, "": "" } questions = self.fetch(api_questions_url, params) yield questions questions_json = json.loads(questions) next_uri = questions_json[''] if not next_uri: break page += 1
return KitsuneClient(self.url, self.archive, from_archive)
item = super().metadata(item, filter_classified=filter_classified) item[''] = item['data'].pop('') return item
try: for x in ['', '', '']: if not data.get(x): raise TypeError("".format(x)) if '' in data: return self.refresh_token(**data) for x in ['', 'code']: if not data.get(x): raise TypeError("".format(x)) return self.get_token(**data) except TypeError as exc: self._handle_exception(exc) return self._make_json_error_response('') except StandardError as exc: self._handle_exception(exc) return self._make_json_error_response('')
params = utils.url_query_params(uri) try: if '' not in params: raise TypeError('') if '' not in params: raise TypeError('') if '' not in params: raise TypeError('') return self.get_authorization_code(**params) except TypeError as exc: self._handle_exception(exc) err = '' if '' in params: u = params[''] return self._make_redirect_error_response(u, err) else: return self._invalid_redirect_uri_response() except StandardError as exc: self._handle_exception(exc) err = '' u = params[''] return self._make_redirect_error_response(u, err)
return self._make_json_error_response('')
if grant_type != '': return self._make_json_error_response('') is_valid_client_id = self.validate_client_id(client_id) is_valid_client_secret = self.validate_client_secret(client_id, client_secret) is_valid_redirect_uri = self.validate_redirect_uri(client_id, redirect_uri) scope = params.get('', '') is_valid_scope = self.validate_scope(client_id, scope) data = self.from_authorization_code(client_id, code, scope) is_valid_grant = data is not None if not (is_valid_client_id and is_valid_client_secret): return self._make_json_error_response('') if not is_valid_grant or not is_valid_redirect_uri: return self._make_json_error_response('') if not is_valid_scope: return self._make_json_error_response('') self.discard_authorization_code(client_id, code) access_token = self.generate_access_token() token_type = self.token_type expires_in = self.token_expires_in refresh_token = self.generate_refresh_token() self.persist_token_information(client_id=client_id, scope=scope, access_token=access_token, token_type=token_type, expires_in=expires_in, refresh_token=refresh_token, data=data) return self._make_json_response({ '': access_token, '': token_type, '': expires_in, '': refresh_token })
return 0
logger = logging.getLogger(__name__) logger.exception(exc)
params['code'] = code if '' not in params: params[''] = self.default_grant_type params.update({'': self.client_id, '': self.client_secret, '': self.redirect_uri}) response = self.http_post(self.token_uri, params) try: return response.json() except TypeError: return response.json
return dict(urlparse.parse_qsl(urlparse.urlparse(url).query, True))
self.places = ctllib.Places(config='', messages='') def _cleanup(): for d in self.places: if os.path.exists(d): shutil.rmtree(d) _cleanup() self.addCleanup(_cleanup) for d in self.places: os.mkdir(d)
self.parser = ctllib.PARSER self.base = ['', '', '', '']
with io.open(fname, "r", encoding='utf-8') as fp: return json.loads(fp.read())
self.events.append(('', name))
DirectoryBasedTest.setUp(self) self.receiver = EventRecorder() self.monitor = directory_monitor.checker(self.testDirectory, self.receiver) self.assertFalse(self.receiver.events)
def _cleanup(testDir): if os.path.exists(testDir): shutil.rmtree(testDir) self.testDirs = {} for subd in ['', '']: testDir = self.testDirs[subd] = os.path.join(os.getcwd(), subd) self.addCleanup(_cleanup, testDir) _cleanup(testDir) os.makedirs(testDir) self.my_reactor = test_procmon.DummyProcessReactor() self.service = service.get(self.testDirs[''], self.testDirs[''], 5, reactor=self.my_reactor) self._finishSetUp()
self.content = content
d = defer.Deferred() self.calls.append((method, url, headers, body)) self.pending[url].append(d) return d
return 'U+0020'.join('U+0020'.join('' % (key, vpart) for vpart in value.split()) for key, value in six.iteritems(self.args)).split()
path = filepath.FilePath(location) files = set() filesContents = {} def _check(path): currentFiles = set(fname for fname in os.listdir(location) if not fname.endswith('')) removed = files - currentFiles added = currentFiles - files for fname in added: contents = path.child(fname).getContent() filesContents[fname] = contents receiver.add(fname, contents) for fname in removed: receiver.remove(fname) same = currentFiles & files for fname in same: newContents = path.child(fname).getContent() oldContents = filesContents[fname] if newContents == oldContents: continue receiver.remove(fname) filesContents[fname] = newContents receiver.add(fname, newContents) files.clear() files.update(currentFiles) return functools.partial(_check, path)
path = filepath.FilePath(location) def _check(path): messageFiles = path.globChildren('*') for message in messageFiles: if message.basename().endswith(''): continue receiver.message(message.getContent()) message.remove() return functools.partial(_check, path)
if self.closed: raise ValueError("") self._maybeReset() if self.url is None: return False return self._maybeCheck()
self.bad += 1
self.bad = 0
restarter, path = beatcheck.parseConfig(opt) pool = client.HTTPConnectionPool(reactor) agent = client.Agent(reactor=reactor, pool=pool) settings = Settings(reactor=reactor, agent=agent) states = {} checker = functools.partial(check, settings, states, path) httpcheck = tainternet.TimerService(opt[''], run, restarter, checker) httpcheck.setName('') return heart.wrapHeart(httpcheck)
for bad in checker(): restarter(bad)
deferred = defer.Deferred() protocol = ProcessProtocol(deferred) process = reactor.spawnProcess(protocol, args[0], args, env=os.environ) def _logEnded(err): err.trap(tierror.ProcessDone, tierror.ProcessTerminated) print(err.value) deferred.addErrback(_logEnded) def _cancelTermination(dummy): for termination in terminations: if termination.active(): termination.cancel() deferred.addCallback(_cancelTermination) terminations = [] terminations.append(reactor.callLater(timeout, process.signalProcess, "")) terminations.append(reactor.callLater(timeout+grace, process.signalProcess, "")) return deferred
pass
pass
if myEnv is None: myEnv = buildEnv() oldEnviron = os.environ def _cleanup(): os.environ = oldEnviron case.addCleanup(_cleanup) os.environ = myEnv
heartSer = makeService() if heartSer is None: return heartSer.setName('') heartSer.setServiceParent(master)
contents = json.loads(contents.decode('utf-8')) tp = contents['type'] if tp == '': self.monitor.stopProcess(contents['name']) log.msg("", contents['name']) elif tp == '': self.monitor.restartAll() log.msg("") elif tp == '': log.msg("", contents['']) for name in self._groupToProcess[contents['']]: log.msg("", name) self.monitor.stopProcess(name) else: raise ValueError('', contents)
self.monitor.removeProcess(name) log.msg("", name) for group in self._processToGroups.pop(name): self._groupToProcess[group].remove(name)
ns = PARSER.parse_args(argv[1:]) call(ns)
content = _dumps(dict(type='', name=name)) _addMessage(places, content)
restarter, path = parseConfig(opt) now = time.time() checker = functools.partial(check, path, now) beatcheck = tainternet.TimerService(opt[''], run, restarter, checker, time.time) beatcheck.setName('') return heart.wrapHeart(beatcheck)
for bad in checker(timer()): restarter(bad)
places = ctllib.Places(config=opt[''], messages=opt['']) restarter = functools.partial(ctllib.restart, places) path = filepath.FilePath(opt['']) return restarter, path
return Hash32(keccak(data))
datastore = current_app.extensions[''].datastore receiver = datastore.get_user(users['']['id']) sender = datastore.get_user(users['']['id']) return AccessRequest.create( recid=pid_value, receiver=receiver, sender_full_name="", sender_email="", sender=sender if confirmed else None, justification="", )
